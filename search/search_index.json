{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"KubeElasti <p>Serverless for Kubernetes</p> <p>Automatically scale your services to zero when idle and scale up when traffic arrives.          KubeElasti saves cost using scale-to-zero without losing any traffic, requires no code changes, and integrates with your existing Kubernetes infrastructure.</p> Get Started Join our community Key Features \ud83d\udcb0 Cost Optimization <p>Scale to zero when there's no traffic to save resources and reduce costs</p> \u26a1 Zero Downtime <p>Queues requests during scale-up to ensure no traffic is lost</p> \ud83d\udd27 Simple Configuration <p>Easy setup with a single CRD and minimal configuration required</p> \ud83d\udd04 Seamless Compatibility <p>Works with your existing Kubernetes setup, HPA, and Keda</p> \ud83d\udcc8 Out of Box Monitoring <p>Built-in monitoring with Prometheus metrics and Grafana dashboards</p> \ud83d\udee1\ufe0f Request Preservation <p>Ensures all incoming requests are processed even during scale operations</p> How It Works 1 Scaling Down <p>When all triggers indicate inactivity, KubeElasti scales the service to 0 replicas and switches to proxy mode</p> 2 Traffic Queueing <p>In proxy mode, KubeElasti intercepts and queues incoming requests to the scaled-down service</p> 3 Scaling Up <p>When traffic arrives, KubeElasti immediately scales the service back up to its minimum replicas</p> 4 Serve Mode <p>Once the service is up, KubeElasti switches to serve mode and processes all queued requests</p> Serverless with just 1 File <pre><code> \n# Create ElastiService CRD for the service you want to optimize\n# Replace values between &lt;&gt; with actual values\nkubectl apply -f - &lt;&lt;EOF \napiVersion: elasti.truefoundry.com/v1alpha1\nkind: ElastiService\nmetadata:\n  name: &lt;TARGET_SERVICE&gt;\n  namespace: &lt;TARGET_SERVICE_NAMESPACE&gt;\nspec:\n  minTargetReplicas: 1\n  service: &lt;TARGET_SERVICE_NAME&gt;\n  cooldownPeriod: 5\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: deployments\n    name: &lt;TARGET_DEPLOYMENT_NAME&gt;\n  triggers:\n    - type:   prometheus\n      metadata:\n        # Select a trigger metric to monitor\n        query: sum(rate(nginx_ingress_controller_nginx_process_requests_total[1m])) or vector(0)\n        # Replace with the address of your Prometheus server\n        serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090\n        threshold: \"0.5\"\nEOF\n\n# \ud83c\udf89 That's it! You just created a scale-to-zero service\n</code></pre> <p>KubeElasti is easy to set up and configure. Follow our step-by-step guide to get started.</p> Full Installation Guide Demo - See KubeElasti in action! Join Our Community <p>Get help, share your experience, and contribute to KubeElasti</p> \ud83d\udc31 GitHub \ud83d\udcac Discord \ud83c\udd98 Report Issues Ready to optimize your Kubernetes resources? Get Started with KubeElasti"},{"location":"blog/2025/08/02/scale-to-zero/","title":"Scale-to-Zero in Kubernetes: Save Costs Without Losing Traffic","text":"<p>If you've ever deployed HTTP services on Kubernetes, you've probably dealt with idle pods that burn resources during off-hours or inactivity. </p> <p>In today's blog, we dive into the concept of Scale-to-Zero, why it matters, how existing tools implement it (and where they fall short), and how KubeElasti solves this problem with zero rewrites, zero request loss, and zero lingering proxies.</p>"},{"location":"blog/2025/08/02/scale-to-zero/#what-is-scale-to-zero","title":"What is Scale-to-Zero?","text":"<p>Scale-to-Zero refers to the ability to automatically scale down a deployment to zero replicas \u2014 effectively turning off the service \u2014 when it's idle, and scaling it back up when traffic resumes.</p> <p>This is ideal for:</p> <ul> <li>Internal or development environments</li> <li>Spiky workloads</li> <li>Scheduled batch jobs</li> <li>Cost-sensitive services (e.g., licensed software, GPU workloads)</li> </ul>"},{"location":"blog/2025/08/02/scale-to-zero/#why-use-scale-to-zero","title":"Why Use Scale-to-Zero?","text":"<p>There are many reasons to use scale-to-zero:</p> <ul> <li>Cost savings: You're always paying for at least one pod</li> <li>Cold-start savings: Cold-start savings are left on the table</li> <li>Infrequently accessed services: Infrequently accessed services still waste memory and CPU</li> </ul> <p>Scale-to-Zero solves this by removing the pod entirely. But... it comes with a catch: if traffic arrives and no pod is running, you'll get a 503 error unless something handles the scale-up and request buffering.</p>"},{"location":"blog/2025/08/02/scale-to-zero/#scale-to-zero-architectural-view","title":"Scale-to-Zero: Architectural View","text":"<p>Here's how a scale-to-zero system typically works:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Proxy\n    participant Operator\n    participant Deployment\n    participant Pod\n\n    User-&gt;&gt;Proxy: HTTP Request\n    Proxy--&gt;&gt;Operator: Pod is down, trigger scale-up\n    Operator-&gt;&gt;Deployment: Scale replicas = 1\n    Deployment-&gt;&gt;Pod: Start new pod\n    Proxy--&gt;&gt;Pod: Forward request after pod is ready</code></pre>"},{"location":"blog/2025/08/02/scale-to-zero/#challenges","title":"Challenges","text":"<ul> <li>Traffic can arrive before the pod is ready.</li> <li>The request may time out unless it's queued.</li> <li>Proxy needs to exit the path once pod is alive (for performance).</li> </ul>"},{"location":"blog/2025/08/02/scale-to-zero/#how-kubeelasti-solves-scale-to-zero","title":"How KubeElasti Solves Scale-to-Zero","text":"<p>KubeElasti is a Kubernetes-native controller + proxy that adds scale-to-zero to your existing HTTP services \u2014 without any rewrites, packaging changes, or vendor lock-in.</p>"},{"location":"blog/2025/08/02/scale-to-zero/#how-it-works","title":"How it works:","text":"<ol> <li>Idle Timeout: If your service sees no traffic for N minutes, the KubeElasti operator scales it to 0.</li> <li>Proxy Intercept: If traffic hits a downed service, the lightweight Elasti proxy queues the request.</li> <li>Scale-Up Trigger: The operator is notified and scales the pod up (via HPA, KEDA, or native scaling).</li> <li>Traffic Replay: Once the pod is ready, the proxy forwards the request and exits the path.</li> </ol> <pre><code>flowchart TB\n  User[User] --&gt;|HTTP Request| ElastiProxy\n  ElastiProxy --&gt;|Pod Not Found| KubeElastiOperator\n  KubeElastiOperator --&gt;|Scale Up| Deployment\n  Deployment --&gt; Pod\n  ElastiProxy --&gt;|Forward| Pod</code></pre>"},{"location":"blog/2025/08/02/scale-to-zero/#what-makes-kubeelasti-different","title":"What Makes KubeElasti Different?","text":"<p>Unlike Knative, KEDA, or OpenFaaS \u2014 which require new runtimes, complex setups, or stay in the path \u2014 KubeElasti is minimal and transparent.</p>"},{"location":"blog/2025/08/02/scale-to-zero/#feature-comparison","title":"Feature Comparison","text":"Feature KubeElasti Knative OpenFaaS KEDA HTTP Add-on Scale to Zero \u2705 \u2705 \u2705 \u2705 Works with Existing Services \u2705 \u274c \u274c \u2705 Request Queueing \u2705 (exits path) \u2705 (stays in path) \u2705 \u2705 (stays in path) Resource Footprint \ud83d\udfe2 Low \ud83d\udd3a High \ud83d\udd39 Medium \ud83d\udfe2 Low Setup Complexity \ud83d\udfe2 Low \ud83d\udd3a High \ud83d\udd39 Medium \ud83d\udd39 Medium"},{"location":"blog/2025/08/02/scale-to-zero/#trade-offs-and-limitations","title":"Trade-offs and Limitations","text":"<p>Like any focused tool, KubeElasti makes some trade-offs:</p> <ul> <li>\u2705 HTTP-only support (for now) \u2014 gRPC/TCP support is in roadmap.</li> <li>\u2705 Only Prometheus metrics are supported for traffic detection.</li> <li>\u2705 Works with Deployments &amp; Argo Rollouts \u2014 more types to come.</li> </ul> <p>That said, it gives you production-ready scale-to-zero in under 5 minutes, with real observability and battle-tested scaling behavior.</p>"},{"location":"blog/2025/08/02/scale-to-zero/#final-thoughts","title":"Final Thoughts","text":"<p>Scale-to-Zero is no longer a \"nice-to-have\" \u2014 it's a cost-saving, resilience-enhancing pattern for modern infrastructure.</p> <p>With KubeElasti, you can implement it without changing your service code, without managing extra FaaS platforms, and without request failures.</p> <p>Want to give it a spin? Start here:</p> <ul> <li>\ud83e\uddea Quickstart: Get Started Guide</li> <li>\ud83e\udde9 Source Code: KubeElasti on GitHub</li> </ul>"},{"location":"src/arch-architecture/","title":"KubeElasti Architecture","text":"<pre><code>graph TB\n    subgraph KubeElasti\n        Controller\n        Resolver\n    end\n\n    Ingress --&gt;|Request| Service\n    Service --&gt;|Active: Pods &gt; 0| Pods\n    Service -.-&gt;|Inactive: Pods = 0| Resolver\n\n    Resolver --&gt;|Inform about the incoming request| Controller\n    Controller --&gt;|Incoming Requests: Scale to 1| Pods\n    Controller --&gt;|No Incoming Requests: Scale to 0| Pods\n    Resolver --&gt;|Forward request| Pods\n</code></pre> <p>KubeElasti comprises two main components: operator and resolver.</p> <ul> <li> <p>Controller/Operator: A Kubernetes controller built using kubebuilder. It monitors ElastiService resources and scales them to 0 or 1 as needed.</p> </li> <li> <p>Resolver: A service that intercepts incoming requests for scaled-down services, queues them, and notifies the elasti-controller to scale up the target service.</p> </li> </ul>"},{"location":"src/arch-architecture/#architecture-serve-mode","title":"Architecture [Serve Mode]","text":"<pre><code>flowchart TB\n  %% === Zones ===\n  LoadGen[[Load Generator]]\n  subgraph INGRESS [\"Ingress\"]\n    Gateway[Gateway]\n  end\n\n  subgraph ElastiPlane [\"KubeElasti\"]\n    Operator[Operator]\n    Resolver[Resolver]\n  end\n\n      ESCRD((ElastiService CRD))\n\nsubgraph Triggers [\"Triggers\"]\n  Prom{{Prometheus}}\nend\n\n\n%%   subgraph ELASTI_CRD [\"ElastiService CRD\"]\n\n%%   end\n\nsubgraph Services\n  TargetSVC{Target-SVC}\nend\n\n  subgraph Endpoints\n    SVC_EPS([EndpointSlice])\n  end\n\n  Pod[[Target Pod]]\n\n  %% === Traffic Flow ===\n  Gateway --&gt;|1: traffic| TargetSVC\n  LoadGen --&gt;|1: traffic| TargetSVC\n  TargetSVC --&gt;|2: Serve Mode| SVC_EPS --&gt; Pod\n\n  %% === Operator Flow ===\n  ESCRD -. \"0: Watch CRD\" .-&gt; Operator\n  Operator --&gt;|4: Scale to 0| Pod\n  Operator --&gt;|3: Poll configured metric every 30 seconds to check if the service can be scaled to 0| Triggers\n  Operator --&gt;|5: Patch CRD to Proxy Mode| ESCRD</code></pre>"},{"location":"src/arch-architecture/#architecture-proxy-mode","title":"Architecture [Proxy Mode]","text":"<pre><code>flowchart TB\n\n  LoadGen[[Load Generator]]\n  subgraph Ingress\n    Gateway[Gateway]\n  end\n\n  subgraph CONTROL_PLANE [\"KubeElasti\"]\n    Operator[Operator]\n    Resolver[Resolver]\n  end\n\n  subgraph Services\n    TargetSVC{Target-SVC}\n    TargetSVC_PVT{Target-SVC-Private}\n  end\n\n  subgraph ENDPOINTS [\"Endpoints\"]\n    ResEPS([to-resolver EndpointSlice])\n  end\n\n\n  subgraph scalers [\"scalers\"]\n    Keda{{\"KEDA\"}}\n    HPA{{\"HPA\"}}\n  end\n\n  Pod[[Target Pod]]\n  ESCRD((ElastiService CRD))\n\n  Gateway --&gt;|1: traffic| TargetSVC\n  LoadGen --&gt;|1: traffic| TargetSVC\n\n  TargetSVC --&gt;|2: Proxy Mode| ResEPS \n\n  %% === Proxy Flow ===\n  ResEPS --&gt;|3: Req| Resolver\n  Resolver -. \"4: Inform about the request\" .-&gt; Operator\n\n  %% === Operator &amp; Control Logic ===\n  Operator --&gt; |5: Request for scale| scalers\n  scalers --&gt; |6: Scale to 1| Pod\n  Operator -. \"7: Watch if scaled to 1\" .-&gt; Pod\n  Operator -. \"8: Switch to Serve Mode\" .-&gt; ESCRD\n  Resolver --&gt;|9: Proxy Request| TargetSVC_PVT \n  TargetSVC_PVT--&gt;|10: Send request, receive response| Pod</code></pre>"},{"location":"src/arch-flow/","title":"Flow Description","text":"<pre><code>graph TB\n  A[\"Steady State (regular traffic flow)\"] --&gt; B[\"Scale to 0: No Traffic\"]\n  B --&gt; C[\"Scale up from 0: New Incoming Traffic\"]\n  C --&gt; A</code></pre> <p>When we enable KubeElasti on a service, the service operates in 3 modes:</p> <ol> <li>Steady State: The service is receiving traffic and doesn't need to be scaled down to 0.</li> <li>Scale Down to 0: The service hasn't received any traffic for the configured duration and can be scaled down to 0.</li> <li>Scale up from 0: The service receives traffic again and can be scaled up to the configured minTargetReplicas.</li> </ol>"},{"location":"src/arch-flow/#1-steady-state-flow-of-requests-to-service","title":"1. Steady State: Flow of requests to service","text":"<p>In this mode, all requests are handled directly by the service pods; the KubeElasti resolver is not involved. The KubeElasti controller continually polls Prometheus with the configured query and checks the result against the threshold value to decide whether the service can be scaled down.</p> <pre><code>---\ntitle: No incoming requests for the configured time period\ndisplayMode: compact\nconfig:\n  layout: elk\n  look: classic\n  theme: dark\n---\n\ngraph LR\n    A[User Request] --&gt; B[Ingress]\n    B --&gt; C[Service]\n    C --&gt;|Active| D[Pods]\n\n    subgraph Elasti Components\n        E[Elasti Controller]\n        F[Elasti Resolver]\n    end\n\n    C -.-&gt;|Inactive| F\n\n    E --&gt;|Poll configured metric every 30 seconds to check if the service can be scaled to 0| S[Prometheus]\n</code></pre>"},{"location":"src/arch-flow/#2-scale-down-to-0-when-there-are-no-requests","title":"2. Scale Down to 0: when there are no requests","text":"<p>If the query from prometheus returns a value less than the threshold, KubeElasti will scale down the service to 0. Before it scales to 0, it redirects all requests to the KubeElasti resolver, then sets the rollout/deployment replicas to 0. It also pauses KEDA (if in use) to prevent it from scaling the service up, because KEDA is configured with <code>minReplicas: 1</code>.</p> <pre><code>---\ntitle: No incoming requests for the configured time period\ndisplayMode: compact\nconfig:\n  layout: elk\n  look: classic\n  theme: dark\n---\n\ngraph LR\n    A[User Request] --&gt; B[Ingress]\n    B --&gt; C[Service]\n\n    subgraph Elasti Components\n        E[Elasti Controller]\n        F[Elasti Resolver]\n    end\n\n    C --&gt;|Active| F\n    E --&gt;|Scale replicas to 0| D[Pods]\n    C -.-&gt;|Inactive| D\n</code></pre>"},{"location":"src/arch-flow/#how-it-works","title":"How it works?","text":""},{"location":"src/arch-flow/#1-switching-to-proxy-mode","title":"1. Switching to Proxy Mode","text":"<p>This is how we decide to switch to proxy mode.</p> <pre><code>sequenceDiagram\n    loop Background Tasks\n    Operator--&gt;&gt;ElastiCRD: Watch CRD for changes in ScaleTargetRef. \n\n    Note right of Operator:  Watch ScaleTargetRef and Triggers\n    Operator--&gt;&gt;TargetService: Watch if ScaleTargetRef is scaled to 0 by &lt;Br&gt;any external component\n    Operator--&gt;&gt;Prometheus: Poll configured metric every 30 seconds&lt;br&gt; to check if the ScaleTargetRef has not received any traffic\n\n    Note right of Operator: If not traffic received for the configured &lt;br&gt; time period, Operator will switch to proxy mode.\n\n    Operator-&gt;&gt;TargetService: Scale replicas to 0\n    Operator-&gt;&gt;ElastiCRD: Switch to proxy mode.\n    end</code></pre>"},{"location":"src/arch-flow/#2-redirecting-requests-to-resolver","title":"2. Redirecting requests to resolver","text":"<p>This is how we redirect requests to resolver.</p> <pre><code>sequenceDiagram\n\nNote right of Operator: When in Proxy Mode\n\n    Operator-&gt;&gt;EndpointSlice: Create EndpointSlice for TargetService&lt;br&gt; which we want to point to resolver POD IPs\n\n    loop Background Tasks\n    Operator--&gt;&gt;Resolver: Watch Resolver POD IPs for changes\n    Operator--&gt;&gt;EndpointSlice: Update EndpointSlice with new POD IPs\n    end</code></pre>"},{"location":"src/arch-flow/#3-sync-private-service-to-public-service","title":"3. Sync Private Service to Public Service","text":"<p>This is how we send traffic to target pod, even if the public service is pointing to resolver. We create a Private Service, as in Proxy Mode, we redirect the traffic to Resolver,  so we need to point the public service to resolver POD IPs.</p> <pre><code>sequenceDiagram\n\nNote right of Operator: When in Proxy Mode\n\n    Operator-&gt;&gt;TargetPrivateService: Create private service\n    loop Background Tasks\n\n    Operator--&gt;&gt;TargetService: Watch changes in label and IPs &lt;br&gt; in public service.\n    Operator--&gt;&gt;TargetPrivateService: Update label and IPs in &lt;Br&gt; private service to match Public Service.\n    end</code></pre>"},{"location":"src/arch-flow/#3-scale-up-from-0-when-the-first-request-arrives","title":"3. Scale up from 0: when the first request arrives","text":"<p>Since the service is scaled down to 0, all requests will hit the KubeElasti resolver. When the first request arrives, KubeElasti will scale up the service to the configured minTargetReplicas. It then resumes Keda to continue autoscaling in case there is a sudden burst of requests. It also changes the service to point to the actual service pods once the pod is up. Requests reaching the KubeElasti resolver are retried for up to five minutes before a response is returned to the client. If the pod takes more than 5 mins to come up, the request is dropped.</p> <pre><code>---\ntitle: First request to pod arrives\ndisplayMode: compact\nconfig:\n  layout: elk\n  look: classic\n  theme: dark\n---\n\ngraph LR\n    A[User Request] --&gt; B[Ingress]\n    B --&gt; C[Service]\n\n    C -.-&gt;|Inactive| F[0 Pods]\n\n    subgraph Elasti Components\n        D[Elasti Controller]\n        E[Elasti Resolver]\n    end\n\n    C --&gt;|Active| E\n    E --&gt;|Hold request in memory and forward once ready| F\n    D --&gt;|Scale replicas up from 0| F\n</code></pre> <pre><code>---\ntitle: State after the first replica is up\ndisplayMode: compact\nconfig:\n  layout: elk\n  look: classic\n  theme: dark\n---\n\ngraph LR\n    A[User] --&gt;|Request| B[Ingress]\n    B --&gt; C[Service]\n\n    subgraph Elasti Components\n        E[Elasti Controller]\n        F[Elasti Resolver]\n    end\n\n    C --&gt;|Active| G[Pods]\n    E --&gt;|Check metric if workload can be scaled to 0| H[Prometheus]\n    C -.- |Inactive| F\n</code></pre>"},{"location":"src/arch-flow/#how-it-works_1","title":"How it works?","text":""},{"location":"src/arch-flow/#1-bring-the-pod-up","title":"1. Bring the pod up","text":"<pre><code>sequenceDiagram    \n    Note right of Operator: When in Proxy Mode\n\n    Gateway-&gt;&gt;TargetService: 1. External or Internal traffic\n    TargetService-&gt;&gt;Resolver: 2. Forward request\n    par \n        Resolver-&gt;&gt;Resolver: 3. Queue requests &lt;br&gt;in-memory (Req remains alive)\n        Resolver-&gt;&gt;Operator: 4. Inform about the incoming request\n    end\n\n    par\n        Operator-&gt;&gt;TargetService: 5. Scale up via HPA or KEDA\n        Operator-&gt;&gt;Resolver: 6. Send info about target private service\n    end\n</code></pre>"},{"location":"src/arch-flow/#2-resolving-queued-requests","title":"2. Resolving queued requests","text":"<pre><code>sequenceDiagram \n    loop\n        Resolver-&gt;&gt;Pod: 7: Check if pod is up\n    end\n\n    par\n        Resolver-&gt;&gt;TargetSvcPvt: 8: Send proxy request\n        TargetSvcPvt-&gt;&gt;Pod: 9: Send &amp; receive req\n    end\n\n    Note right of Resolver: Once pod is up, switch to serve mode\n</code></pre>"},{"location":"src/arch-monitoring/","title":"Monitoring","text":"<p>Set <code>.global.enableMonitoring</code> to <code>true</code> in the values.yaml file to enable monitoring.</p> <p>This will create two ServiceMonitor custom resources to enable Prometheus to discover the KubeElasti components. To verify this, you can open your Prometheus interface and search for metrics prefixed with <code>elasti_</code>, or navigate to the Targets section to check if KubeElasti is listed.</p> <p>Once verification is complete, you can use the provided Grafana dashboard to monitor the internal metrics and performance of KubeElasti.</p> <p></p>"},{"location":"src/arch-operator/","title":"Operator Architecture","text":"<pre><code>\nflowchart LR\n\n%% === API ===\nsubgraph \"API Layer\"\n  CRD[\"ElastiService CRD&lt;br/&gt;api/v1alpha1\"]:::core\n  GroupVersion[\"Group/Version&lt;br/&gt;groupversion_info.go\"]:::core\nend\n\n%% === CONTROLLER ===\nsubgraph \"Controller Layer\"\n  direction TB\n  Reconciler[\"Reconciler&lt;br/&gt;elastiservice_controller.go\"]:::core\n  Lifecycle[\"Lifecycle Mgmt&lt;br/&gt;opsCRD.go\"]:::core\n  Deploy[\"Deploy Mgmt&lt;br/&gt;opsDeployment.go\"]:::core\n  SVCs[\"Service Mgmt&lt;br/&gt;opsServices.go\"]:::core\n  EPSlices[\"EndpointSlice Mgmt&lt;br/&gt;opsEndpointslices.go\"]:::core\n  Rollouts[\"Rollout Mgmt&lt;br/&gt;opsRollout.go\"]:::core\n  Modes[\"Mode Switching&lt;br/&gt;opsModes.go\"]:::core\n  Informers[\"Informer Interface&lt;br/&gt;opsInformer.go\"]:::core\nend\n\nCRD --&gt;|watches| Reconciler\nGroupVersion --&gt; Reconciler\nReconciler --&gt;|manages| Deploy &amp; SVCs &amp; EPSlices &amp; Rollouts &amp; Modes\nReconciler --&gt;|uses| Informers\n\n%% === RESOURCE MGMT ===\nsubgraph \"Resource Management\"\n  CRDReg[\"CRD Registry&lt;br/&gt;crddirectory/\"]:::core\n  Server[\"ElastiServer&lt;br/&gt;elastiserver/\"]:::http\n  Prom[\"Prometheus Client&lt;br/&gt;prom/\"]:::metrics\nend\n\nReconciler --&gt;|updates| CRDReg\nServer --&gt;|scale requests| Reconciler\nProm --&gt;|collects| Reconciler\n\n%% === INFRASTRUCTURE ===\nsubgraph \"Infrastructure &amp; Boot\"\n  Main[\"Entry Point&lt;br/&gt;main.go\"]:::core\n  InfMgr[\"Informer Manager&lt;br/&gt;informer/\"]:::core\nend\n\nMain --&gt;|initializes| InfMgr --&gt;|manages| Informers\n\n%% === OBSERVABILITY ===\nsubgraph \"Observability\"\n  Metrics[\"/metrics endpoint\"]:::metrics\nend\n\nReconciler --&gt;|exposes| Metrics\nProm --&gt;|scrapes| Metrics\n\n%% === DATA FLOW ===\nsubgraph \"Scaling Logic\"\n  ScaleLogic[\"ScaleTargetFromZero\"]:::core\nend\n\nServer --&gt;|trigger scale| ScaleLogic\nReconciler --&gt;|syncs state| ScaleLogic\n\n%% === EXTERNAL ===\nsubgraph \"External Dependencies\"\n  K8s[\"Kubernetes API&lt;br/&gt;client-go\"]:::external\n  Kustomize[\"Kustomize\"]:::external\n  Sentry[\"Sentry\"]:::external\nend\n\nReconciler --&gt;|uses| K8s\nMain --&gt; Kustomize &amp; Sentry\n</code></pre>"},{"location":"src/arch-operator/#high-level-summary","title":"High-Level Summary","text":"<p>The system is a Kubernetes operator that manages <code>ElastiService</code> CRDs, which encapsulate the desired state of elastiservices. It leverages a layered architecture with clear separation of concerns:</p> <ul> <li>API Layer: Defines the CRD schema (<code>api/v1alpha1</code>) with types, versioning, and deep copy functions.</li> <li>Controller Layer: Implements reconciliation logic (<code>internal/controller</code>) to maintain resource state, handle lifecycle events, and coordinate scaling and mode switching.</li> <li>Resource Management: Manages Kubernetes resources such as Deployments, Services, EndpointSlices, and CRDs, ensuring they reflect the desired state.</li> <li>Informers &amp; Watchers: Uses Kubernetes informers (<code>internal/informer</code>) for efficient event-driven updates, with a singleton manager to prevent redundant watches.</li> <li>External Integration: Includes a custom HTTP server (<code>internal/elastiserver</code>) for handling scaling requests from an external resolver, with Sentry for error tracking.</li> <li>Metrics &amp; Observability: Prometheus metrics are integrated for observability, tracking reconciliation durations, CRD updates, informer activity, and scaling events.</li> <li>Deployment &amp; Configuration: Uses Kustomize (<code>config/</code>) for managing deployment manifests, RBAC, CRDs, and monitoring configurations.</li> </ul>"},{"location":"src/arch-resolver/","title":"Resolver Architecture","text":"<pre><code>flowchart LR\n  %% \u2500\u2500 USER &amp; ENTRY \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  User((\"Client\")) --&gt; RP[\"Proxy&lt;br/&gt;:8012\"] --&gt; Main[\"Main&lt;br/&gt;cmd/main.go\"] --&gt; IS[\"Metrics&lt;br/&gt;:8013\"]\n\n  %% \u2500\u2500 CORE MODULES \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  subgraph Mods[\"Core Modules\"]\n    Handler[\"Handler\"]:::core\n    Hosts[\"Hosts\"]:::core\n    Thr[\"Throttle\"]:::core\n    Oper[\"Operator Comm\"]:::core\n    Obs[\"Observability\"]:::core\n  end\n  Main -- uses --&gt; Handler &amp; Hosts &amp; Thr &amp; Oper &amp; Obs\n\n  %% Request flow (compact arrows)\n  Handler --&gt; Hosts\n  Handler --&gt; Thr\n  Thr --&gt; Handler\n  Handler --&gt; Obs\n  Handler -.-&gt; Sentry[\"Sentry\"]\n\n  %% Operator comm\n  Handler -.-&gt; Oper\n  Oper -.-&gt; OpSvc[\"Operator Svc\"]\n\n  %% External deps\n  Thr -.-&gt; K8sAPI[\"K8s API\"]\n  Obs -.-&gt; Prom[\"Prometheus\"]\n</code></pre>"},{"location":"src/comparisons/","title":"Comparisons with Other Solutions","text":"<p>This document compares KubeElasti with other popular serverless and scale-to-zero solutions in the Kubernetes ecosystem.</p>"},{"location":"src/comparisons/#feature-comparison-table","title":"Feature Comparison Table","text":"Feature KubeElasti Knative OpenFaaS KEDA HTTP Add-on Scale to Zero \u2705 \u2705 \u2705 \u2705 Works with Existing Services \u2705 \u274c \u274c \u2705 Resource Footprint \ud83d\udfe2 Low \ud83d\udd3a High \ud83d\udd39 Medium \ud83d\udfe2 Low Request queueing \u2705(Takes itself out of the path) \u2705 (Remains in the path) \u2705 \u2705(Remains in the path) Setup Complexity \ud83d\udfe2 Low \ud83d\udd3a High \ud83d\udd39 Medium \ud83d\udd39 Medium"},{"location":"src/comparisons/#knative","title":"Knative","text":""},{"location":"src/comparisons/#overview","title":"Overview","text":"<p>Knative is a comprehensive platform for deploying and managing serverless workloads on Kubernetes. It provides a complete serverless experience with features like scale-to-zero, request-based autoscaling, and traffic management.</p>"},{"location":"src/comparisons/#key-differences","title":"Key Differences","text":"<ul> <li>Complexity: Knative is a full-featured platform that requires significant setup and maintenance. KubeElasti is focused solely on scale-to-zero functionality and can be added to existing services with minimal configuration.</li> <li>Integration: Knative requires services to be deployed as Knative services. KubeElasti works with existing Kubernetes deployments and Argo Rollouts without modification.</li> <li>Learning Curve: Knative has a steeper learning curve due to its many concepts and components. KubeElasti follows familiar Kubernetes patterns with simple CRD-based configuration.</li> </ul>"},{"location":"src/comparisons/#openfaas","title":"OpenFaaS","text":""},{"location":"src/comparisons/#overview_1","title":"Overview","text":"<p>OpenFaaS is a framework for building serverless functions with Docker and Kubernetes, making it easy to deploy serverless functions to any cloud or on-premises.</p>"},{"location":"src/comparisons/#key-differences_1","title":"Key Differences","text":"<ul> <li>Purpose: OpenFaaS is primarily designed for Function-as-a-Service (FaaS) workloads. KubeElasti is built for existing HTTP services.</li> <li>Architecture: OpenFaaS requires functions to be written and packaged in a specific way. KubeElasti works with any HTTP service without code changes.</li> <li>Scaling: OpenFaaS uses its own scaling mechanisms. KubeElasti integrates with existing autoscalers (HPA/KEDA) while adding scale-to-zero capability.</li> </ul>"},{"location":"src/comparisons/#keda-http-add-on","title":"KEDA HTTP Add-on","text":""},{"location":"src/comparisons/#overview_2","title":"Overview","text":"<p>KEDA HTTP Add-on is an extension to KEDA that enables HTTP-based scaling, including scale-to-zero functionality.</p>"},{"location":"src/comparisons/#key-differences_2","title":"Key Differences","text":"<ul> <li>Request Handling: <ul> <li>KEDA http add-on inserts itself in the http path and handles requests even when the service has been scaled up.</li> <li>KubeElasti takes itself out of the http path once the service has been scaled up.</li> </ul> </li> <li>Integration:<ul> <li>KEDA HTTP Add-on requires KEDA installation and configuration.</li> <li>KubeElasti can work standalone or integrate with KEDA if needed.</li> </ul> </li> </ul>"},{"location":"src/comparisons/#when-to-choose-kubeelasti","title":"When to Choose KubeElasti","text":"<p>KubeElasti is the best choice when you:</p> <ol> <li>Need to add scale-to-zero capability to existing HTTP services</li> <li>Want to ensure zero request loss during scaling operations</li> <li>Prefer a lightweight solution with minimal configuration</li> <li>Need integration with existing autoscalers (HPA/KEDA)</li> </ol>"},{"location":"src/dev-contributors/","title":"Contributors","text":"<p>This page recognizes all the dedicated and incredible people who have contributed to the KubeElasti project. We appreciate all contributions, from code to documentation, testing, and community support.</p> <p></p> <p>Loading contributors...</p>"},{"location":"src/dev-env/","title":"Dev Environment","text":"<p>Setting up your development environment for KubeElasti involves preparing your local setup for building, testing, and contributing to the project. Follow these steps to get started:</p>"},{"location":"src/dev-env/#1-get-required-tools","title":"1. Get required tools","text":"<p>Ensure you have the following tools installed:</p> <ul> <li>Go: The programming language used for KubeElasti. Download and install it from golang.org.</li> <li>Docker: For containerization and building Docker images. Install it from docker.com.</li> <li>kubectl: Command-line tool for interacting with Kubernetes. Install it from kubernetes.io.</li> <li>Helm: Package manager for Kubernetes. Install it from helm.sh.</li> <li>Docker Desktop/Kind/Minikube: A local kubernetes cluster. Make sure you have the local cluster running before development.</li> <li>Make: Helps in working with the project.</li> <li>Istio: Required to test the project with istio. Install from istio.io</li> <li>k6: Required to load test the project. Install from k6.io</li> </ul>"},{"location":"src/dev-env/#2-clone-the-repository","title":"2. Clone the Repository","text":"<p>Clone the KubeElasti repository from GitHub to your local machine:</p> <pre><code>git clone https://github.com/truefoundry/KubeElasti.git\ncd KubeElasti\n</code></pre> <p>Make sure you check out the documentation and architecture before making your changes.</p>"},{"location":"src/dev-env/#3-repository-structure","title":"3. Repository Structure","text":"<p>Understanding the repository structure will help you navigate and contribute effectively to the KubeElasti project. Below is an overview of the key directories and files in the repository:</p> <pre><code>.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 charts\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 go.work\n\u251c\u2500\u2500 go.work.sum\n\u251c\u2500\u2500 kustomization.yaml\n\u251c\u2500\u2500 operator\n\u251c\u2500\u2500 pkg\n\u251c\u2500\u2500 playground\n\u251c\u2500\u2500 resolver\n\u2514\u2500\u2500 test\n</code></pre>"},{"location":"src/dev-env/#main-modules","title":"Main Modules","text":"<ul> <li><code>./operator</code>: Contains the code for Kubernetes operator, created using kubebuilder.   <pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 api\n\u251c\u2500\u2500 cmd\n\u251c\u2500\u2500 config\n\u251c\u2500\u2500 go.mod\n\u251c\u2500\u2500 go.sum\n\u251c\u2500\u2500 internal\n\u2514\u2500\u2500 test\n</code></pre></li> <li><code>./api</code>: Contains the folder named after the apiVersion, and has custom resource type description.</li> <li><code>./config</code>: Kubernetes manifest files.</li> <li><code>./cmd</code>: Main files for the tool.</li> <li><code>./internal</code>: Internal packages of the program.</li> <li><code>./Makefile</code>: Helps with working with the program. Use <code>make help</code> to see all the available commands.</li> <li><code>./resolver</code>: Contains the code for resolver.</li> <li>File structure of it is similar to that of Operator.</li> </ul>"},{"location":"src/dev-env/#other-directories","title":"Other Directories","text":"<ul> <li><code>./playground</code>: Code to setup a playground to try and test KubeElasti.</li> <li><code>./test</code>: Load testing scripts.</li> <li><code>./pkg</code>: Common packages, shared by Operator and Resolve.</li> <li><code>./charts</code>: Helm chart template.</li> <li><code>./docs</code>: Detailed documentation on the HLD, LLD and Architecture of KubeElasti.</li> </ul>"},{"location":"src/dev-playground/","title":"Playground","text":"<p>This guide will help you set up a local Kubernetes cluster to test the elasti operator and resolver. Follow these steps from the project home directory.</p>"},{"location":"src/dev-playground/#1-local-cluster","title":"1. Local Cluster","text":"<p>If you don't already have a local Kubernetes cluster, you can set one up using Minikube, Kind or Docker-Desktop:</p> KindMinikubeDocker-Desktop <pre><code>kind create cluster --config ./playground/infra/kind-config.yaml\n</code></pre> <pre><code>minikube start\n</code></pre> <p>Enable it in Docker-Desktop</p>"},{"location":"src/dev-playground/#2-start-a-local-docker-registry","title":"2. Start a Local Docker Registry","text":"<p>Run a local Docker registry container, to push our images locally and access them in our cluster.</p> <pre><code>docker run -d --restart=always -p 5001:5000 --name kind-registry registry:2;\n\n# If you are using kind, connect the registry to kind network\ndocker network create kind\ndocker network connect \"kind\" kind-registry\n</code></pre> <p>Add registry to Minikube and Kind</p> <p>You will need to add this registry to Minikube and Kind. With Docker-Desktop, it is automatically picked up.</p> <p>In MacOS, 5000 is not available, so we use 5001 instead.</p>"},{"location":"src/dev-playground/#3-build-publish-resolver-operator","title":"3. Build &amp; Publish Resolver &amp; Operator","text":"<p>Once you have made the necessary changes to the resolver or operator, you can build and publish it using the following commands:</p> <pre><code>make -C resolver docker-build docker-push IMG=localhost:5001/elasti-resolver:v1alpha1\nmake -C operator docker-build docker-push IMG=localhost:5001/elasti-operator:v1alpha1\n</code></pre> <p>Make sure you have configured the local context in kubectl. With kind, it is automatically picked up,</p>"},{"location":"src/dev-playground/#4-setup-prometheus","title":"4. Setup Prometheus","text":"<p>We will setup a sample prometheus to read metrics from the ingress controller.</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --set alertmanager.enabled=false \\\n  --set grafana.enabled=false \\\n  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false\n</code></pre>"},{"location":"src/dev-playground/#5-install-ingress-controller","title":"5. Install Ingress Controller","text":"NGINXIstio <p>Install the NGINX Ingress Controller using Helm:   <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm upgrade --install nginx-ingress ingress-nginx/ingress-nginx \\\n  --namespace nginx \\\n  --set controller.metrics.enabled=true \\\n  --set controller.metrics.serviceMonitor.enabled=true \\\n  --create-namespace\n</code></pre></p> <p>This will deploy a nginx ingress controller in the <code>ingress-nginx</code> namespace.</p> <p>Install the Istio Ingress Controller using Helm:   <pre><code># Download the latest Istio release from the official Istio website.\ncurl -L https://istio.io/downloadIstio | sh -\n# Move it to home directory\nmv istio-x.xx.x ~/.istioctl\nexport PATH=$HOME/.istioctl/bin:$PATH\n\nistioctl install --set profile=default -y\n\n# Label the namespace where you want to deploy your application to enable Istio sidecar Injection\nkubectl create namespace target\nkubectl label namespace target istio-injection=enabled\n\n# Create a gateway\nkubectl apply -f ./playground/config/gateway.yaml\n</code></pre></p> <p>This will deploy a Istio ingress controller in the <code>istio-system</code> namespace.</p>"},{"location":"src/dev-playground/#6-deploy-kubeelasti-locally","title":"6. Deploy KubeElasti Locally","text":"<p>We will be using <code>playground/infra/elasti-demo-values.yaml</code> for the helm installation. Configure the image uri according to the requirement. Post that follow below steps from the project home directory:</p> <pre><code>kubectl create namespace elasti\nhelm upgrade --install elasti ./charts/elasti -n elasti -f ./playground/infra/elasti-demo-values.yaml\n</code></pre> <p>If you want to enable monitoring, please make <code>enableMonitoring</code> true in the values file.</p>"},{"location":"src/dev-playground/#7-deploy-a-demo-service","title":"7. Deploy a demo service","text":"<p>Run a demo application in your cluster. We will use a sample httpbin service to demonstrate how to configure a service to handle its traffic via elasti.</p> <pre><code>kubectl create namespace target\nkubectl apply -f ./playground/config/demo-application.yaml -n target\n</code></pre> <p>Add virtual service if you are using istio.</p> <pre><code># ONLY IF YOU ARE USING ISTIO\n# Create a Virtual Service to expose the demo service\nkubectl apply -f ./playground/config/demo-virtualService.yaml\n</code></pre> <p>This will deploy a httpbin service in the <code>target</code> namespace.</p>"},{"location":"src/dev-playground/#8-create-elastiservice-resource","title":"8. Create ElastiService Resource","text":"<p>Using the ElastiService Definition, create a manifest file for your service and apply it. For demo, we use the below manifest.</p> <pre><code>kubectl -n target apply -f ./playground/config/demo-elastiService.yaml\n</code></pre>"},{"location":"src/dev-playground/#9-test-the-service","title":"9. Test the service","text":""},{"location":"src/dev-playground/#91-scale-down-the-service","title":"9.1 Scale down the service","text":"<pre><code>kubectl -n target scale deployment httpbin --replicas=0\n</code></pre>"},{"location":"src/dev-playground/#92-send-request-to-the-service-while-target-is-scaled-down","title":"9.2 Send request to the service while target is scaled down","text":"<pre><code>kubectl run -it --rm curl --image=alpine/curl -- http://httpbin.target.svc.cluster.local/headers\n</code></pre>"},{"location":"src/dev-playground/#93-portforward-ingress","title":"9.3 Portforward Ingress","text":"<pre><code>kubectl port-forward svc/nginx-ingress-controller 8080:80 -n ingress-nginx\n\n# or \n\nkubectl port-forward -n istio-system svc/istio-ingressgateway 8080:80\n</code></pre> <p>You should see the target service pod getting scaled up and response from the new pod.</p>"},{"location":"src/dev-playground/#10-debug-tips","title":"10. Debug tips","text":""},{"location":"src/dev-playground/#101-watch-logs","title":"10.1 Watch logs","text":"<pre><code>kubectl logs -n elasti deployments/elasti-operator-controller-manager -f\nkubectl logs -n elasti deployments/elasti-resolver -f\nkubectl logs -n target deployments/httpbin -f\nkubectl logs -n istio-system deployments/istiod -f\n</code></pre>"},{"location":"src/dev-playground/#10-delete-kubeelasti","title":"10. Delete KubeElasti","text":"<pre><code>helm delete elasti -n elasti\n</code></pre>"},{"location":"src/dev-playground/#11-redeploy-kubeelasti","title":"11. Redeploy KubeElasti","text":"<p>In case you want to redeploy KubeElasti, with your latest changes, you can use the below command:</p>"},{"location":"src/dev-playground/#111-delete-the-elastiservice","title":"11.1 Delete the ElastiService","text":"<pre><code>kubectl -n target delete -f ./playground/config/demo-elastiService.yaml\n</code></pre>"},{"location":"src/dev-playground/#112-delete-the-kubeelasti","title":"11.2 Delete the KubeElasti","text":"<pre><code>helm delete elasti -n elasti\n</code></pre>"},{"location":"src/dev-playground/#113-remove-old-images","title":"11.3 Remove old images","text":"<pre><code>docker rmi localhost:5001/elasti-resolver:v1alpha1\n\ndocker rmi localhost:5001/elasti-operator:v1alpha1\n</code></pre> <p>Post this, repeat steps 6 and 8, to redeploy KubeElasti.</p>"},{"location":"src/dev-test-e2e/","title":"End-to-End Testing","text":"<p>Use the KUTTL framework to execute KubeElasti end-to-end tests in a real Kubernetes environment:</p> <pre><code>cd ./tests/e2e\nmake setup   # Sets up environment\nmake test    # Runs tests\n</code></pre> <p>For detailed information about the E2E test framework, see tests/e2e/README.md.</p>"},{"location":"src/dev-test-e2e/#testing-flow","title":"Testing Flow","text":"<p>The E2E testing pipeline follows these detailed steps:</p> <ol> <li> <p>Registry Setup</p> <ol> <li>A Docker registry is started on <code>port 5002</code></li> <li>This registry will store the locally built operator and resolver images</li> </ol> </li> <li> <p>Kind Cluster Creation</p> <ol> <li>A Kind cluster is created using the configuration in <code>kind-config.yaml</code></li> <li>The registry is connected to the Kind network to allow image pulling</li> </ol> </li> <li> <p>Image Building and Publishing</p> <ol> <li>Elasti operator and resolver images are built from source</li> <li>Images are tagged and pushed to the local registry</li> </ol> </li> <li> <p>Dependency Installation</p> <ol> <li>Istio Ingress: Sets up the ingress gateway for routing external traffic</li> <li>Prometheus: Installed for metrics collection (without Grafana to reduce overhead)</li> <li>KEDA: Installed for event-driven autoscaling capabilities</li> <li>KubeElasti: The operator and CRDs are installed using Helm</li> </ol> </li> <li> <p>Test Initialization</p> <ol> <li>Persistent, lightweight resources are applied via the <code>kuttl-test.yaml</code> config</li> <li>These resources are shared across all test cases</li> </ol> </li> <li> <p>Test Execution</p> <ol> <li>Tests are executed in order based on their numerical prefix</li> <li>Each folder in the <code>tests/</code> directory represents an independent test</li> <li>Each file within a test folder represents steps of that test</li> <li>Tests can run in parallel if enabled, with no cross-dependencies between test folders</li> </ol> </li> <li> <p>Test Cleanup</p> <ol> <li>Resources are cleaned up after each test completes</li> </ol> </li> </ol> <pre><code>graph TD\n    A[Start Registry:5002] --&gt; B[Create Kind Cluster]\n    B --&gt; C[Build &amp; Publish Images to Registry]\n    C --&gt; D[Install Dependencies]\n\n    subgraph \"Dependencies Setup\"\n    D --&gt; D1[Install Istio Ingress]\n    D --&gt; D2[Install Prometheus]\n    D --&gt; D3[Install KEDA]\n    D --&gt; D4[Install Elasti]\n    end\n\n    D1 &amp; D2 &amp; D3 &amp; D4 --&gt; E[Apply KUTTL Config Resources]\n    E --&gt; F[Run KUTTL Tests]\n    F --&gt; G1[Test 00-Scenario-x]\n    F --&gt; Ga1[Test 01-Scenario-y]\n    F --&gt; Gb1[Test 02-Scenario-z]\n\n    subgraph \"Parallel 0 Test Execution\"\n    G1 --&gt; G2[01-Step]\n    G2 --&gt; G3[02-Step]\n    G3 --&gt; G4[03-Step]\n    end\n\n    subgraph \"Parallel 1 Test Execution\"\n    Ga1 --&gt; Ga2[01-Step]\n    Ga2 --&gt; Ga3[02-Step]\n    Ga3 --&gt; Ga4[03-Step]\n    end\n\n    subgraph \"Parallel 2 Test Execution\"\n    Gb1 --&gt; Gb2[01-Step]\n    Gb2 --&gt; Gb3[02-Step]\n    Gb3 --&gt; Gb4[03-Step]\n    end\n\n    G4 --&gt; H[Cleanup Resources]\n    Ga4 --&gt; H\n    Gb4 --&gt; H</code></pre>"},{"location":"src/dev-test-e2e/#test-configuration","title":"Test configuration","text":"<p>The test configuration is defined in the <code>kuttl-test.yaml</code> file. There is a <code>timeout</code> field in the <code>kuttl-test.yaml</code> file, which is set to 30s by default. This is the timeout for each test step.</p>"},{"location":"src/dev-test-e2e/#testing-environment","title":"Testing Environment","text":"<p>The testing environment consists of:</p> <pre><code>flowchart TB\n    subgraph \"Kind Cluster\"\n        subgraph \"elasti namespace\"\n            OP[Elasti Operator]\n            CRD[Elasti CRDs]\n        end\n\n        subgraph \"monitoring namespace\"\n            PROM[Prometheus Server]\n        end\n\n        subgraph \"default namespace\"\n            D[Test Deployment]\n            ES[ElastiService CR]\n            TF[Traffic Generator]\n        end\n\n        PROM -- metrics --&gt; OP\n        OP -- monitors --&gt; ES\n        ES -- controls --&gt; D\n        TF -- generates traffic --&gt; D\n    end</code></pre>"},{"location":"src/dev-test-e2e/#adding-new-tests","title":"Adding New Tests","text":"<p>To add a new test scenario:</p> <ol> <li>Create a new directory in the <code>tests/e2e/tests/</code> directory following the KUTTL format, <code>&lt;number&gt;-&lt;test-name&gt;</code>.</li> <li>Define test steps with commands and assertions, in the directory using <code>&lt;number&gt;-&lt;step-name&gt;.yaml</code> format.</li> <li>Add any supporting files or manifests needed</li> <li>To run the test individually, use:    <pre><code>make test T=&lt;number&gt;-&lt;test-name&gt;\n</code></pre></li> </ol>"},{"location":"src/dev-test-e2e/#kuttl-test-files-structure","title":"KUTTL Test Files Structure","text":"<p>KUTTL tests follow a specific structure:</p> <pre><code>tests/\n\u2514\u2500\u2500 00-elasti-setup/            # Test case (folder named with numbered prefix)\n    \u251c\u2500\u2500 00-apply.yaml           # First step - apply resources, created the required scenario.\n    \u251c\u2500\u2500 01-wait.yaml            # Second step - wait for resources to be ready.\n    \u2514\u2500\u2500 02-assert.yaml          # Third step - assertion\n</code></pre> <ul> <li>Each directory represents an individual test (This number doesn't determine order, that works only for steps inside the directory)</li> <li>Each file within a test directory represents a step in that test</li> <li>Steps follow naming convention with prefix 00-, 01-, etc. for execution ordering<ul> <li>For example, <code>00-assert.yaml</code> is the first step in the test, <code>01-apply.yaml</code> is the second step, and so on.</li> <li>Same doesn't apply for folders in <code>tests/e2e/tests/</code> directory. We still follow the same naming for them just to keep it consistent.</li> </ul> </li> </ul>"},{"location":"src/dev-test-e2e/#run-single-test","title":"Run Single Test","text":"<pre><code>make test T=00-elasti-setup\n</code></pre>"},{"location":"src/dev-test-e2e/#example-test-structure","title":"Example Test Structure","text":"<pre><code># Test step to assert elasti operator, resolver and target deployment are running, and if elasti service is in serve mode.\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: elasti-operator-controller-manager\n  namespace: elasti\nstatus:\n  readyReplicas: 1\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: elasti-resolver\n  namespace: elasti\nstatus:\n  readyReplicas: 1\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: target-deployment\n  namespace: default\nstatus:\n  readyReplicas: 1\n---\napiVersion: elasti.truefoundry.com/v1alpha1\nkind: ElastiService\nmetadata:\n  name: target-elastiservice\n  namespace: default\nstatus:\n  mode: serve\n</code></pre> <p>Tip</p> <p>Refer to KUTTL Docs for more information.</p>"},{"location":"src/dev-test-e2e/#tips-for-writing-kuttl-tests","title":"Tips for Writing KUTTL Tests","text":"<ol> <li>Naming Convention: Use numerical prefixes within test folders to know the sequence of test cases and files to control execution of the steps.</li> <li>Avoid Cross-Dependencies: Each test folder should be independent of others</li> <li>Use Timeouts Wisely: Set appropriate timeouts for operations that may take time</li> <li>Resource Sharing: Put shared resources in <code>kuttl-test.yaml</code> commands section</li> <li>Debugging: Use <code>kubectl kuttl test --debug</code> for verbose output during test development</li> </ol>"},{"location":"src/dev-test-load/","title":"Load testing","text":""},{"location":"src/dev-test-load/#1-update-k6-tests","title":"1. Update k6 tests","text":"<p>Update <code>./test/load.js</code> to set your target URL and adjust any other configuration values.</p>"},{"location":"src/dev-test-load/#2-run-loadjs","title":"2. Run load.js","text":"<p>Run the following command to run the test.</p> <pre><code>chmod +x ./test/generate_load.sh\ncd ./test\n./generate_load.sh\n</code></pre>"},{"location":"src/dev-test-monitoring/","title":"Test Monitoring","text":"<pre><code># First, add the prometheus-community Helm repository.\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\n# Install the kube-prometheus-stack chart. This chart includes Prometheus and Grafana.\nkubectl create namespace prometheus\nhelm install prometheus-stack prometheus-community/kube-prometheus-stack -n prometheus\n\n# Port-forward to access the dashboard\nkubectl port-forward -n prometheus services/prometheus-stack-grafana 3000:80\n\n# Get the admin user.\nkubectl get secret --namespace prometheus prometheus-stack-grafana -o jsonpath=\"{.data.admin-user}\" | base64 --decode ; echo\n# Get the admin password.\nkubectl get secret --namespace prometheus prometheus-stack-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre> <p>After this, you can use <code>./playground/infra/elasti-dashboard.yaml</code> to import the KubeElasti dashboard.</p>"},{"location":"src/gs-configure-elastiservice/","title":"Configure ElastiService","text":"<p>To enable scale to 0 on any deployment, we will need to create an ElastiService custom resource for that deployment. </p> <p>An ElastiService custom resource has the following structure:</p> elasti-service.yaml<pre><code>apiVersion: elasti.truefoundry.com/v1alpha1\nkind: ElastiService\nmetadata:\n  name: &lt;service-name&gt; # (1)\n  namespace: &lt;service-namespace&gt; # (2)\nspec:\n  minTargetReplicas: &lt;min-target-replicas&gt; # (3)\n  service: &lt;service-name&gt;\n  cooldownPeriod: &lt;cooldown-period&gt; # (4)\n  scaleTargetRef:\n    apiVersion: &lt;apiVersion&gt; # (5)\n    kind: &lt;kind&gt; # (6)\n    name: &lt;deployment-or-rollout-name&gt; # (7)\n  triggers:\n  - type: &lt;trigger-type&gt; # (8)\n    metadata:\n      query: &lt;query&gt; # (9)\n      serverAddress: &lt;server-address&gt; # (10)\n      threshold: &lt;threshold&gt; # (11)\n      uptimeFilter: &lt;uptime-filter&gt; #(12)\n  autoscaler:\n    name: &lt;autoscaler-object-name&gt; # (13)\n    type: &lt;autoscaler-type&gt; # (14)\n</code></pre> <ol> <li>Replace it with the service you want managed by elasti.</li> <li>Replace it with the namespace of the service.</li> <li>Replace it with the min replicas to bring up when first request arrives. Minimum: 1</li> <li>Replace it with the cooldown period to wait after scaling up before considering scale down. Default: 900 seconds (15 minutes) | Maximum: 604800 seconds (7 days) | Minimum: 1 second (1 second)</li> <li>ApiVersion should be <code>apps/v1</code> if you are using deployments or <code>argoproj.io/v1alpha1</code> in case you are using argo-rollouts. </li> <li>Kind should be either <code>Deployment</code> or <code>Rollout</code> (in case you are using Argo Rollouts).</li> <li>Name should exactly match the name of the deployment or rollout.</li> <li>Replace it with the trigger type. Currently, KubeElasti supports only one trigger type - <code>prometheus</code>. </li> <li>Replace it with the trigger query. In this case, it is the number of requests per second.</li> <li>Replace it with the trigger server address. In this case, it is the address of the prometheus server.</li> <li>Replace it with the trigger threshold. In this case, it is the number of requests per second.</li> <li>Replace it with the uptime filter of your TSDB instance. Default: <code>container=\"prometheus\"</code>.</li> <li>Replace it with the autoscaler name. In this case, it is the name of the KEDA ScaledObject.</li> <li>Replace it with the autoscaler type. In this case, it is <code>keda</code>.</li> </ol> <p>The key fields to be specified in the spec are:</p> <ul> <li><code>&lt;service-name&gt;</code>: Replace it with the service you want managed by elasti.</li> <li><code>&lt;service-namespace&gt;</code>: Replace by namespace of the service.</li> <li><code>&lt;min-target-replicas&gt;</code>: Min replicas to bring up when first request arrives.<ul> <li>Minimum: 1</li> </ul> </li> <li><code>&lt;scaleTargetRef&gt;</code>: Reference to the scale target similar to the one used in HorizontalPodAutoscaler.</li> <li><code>&lt;kind&gt;</code>: Replace by <code>rollouts</code> or <code>deployments</code></li> <li><code>&lt;apiVersion&gt;</code>: Replace with <code>argoproj.io/v1alpha1</code> or <code>apps/v1</code></li> <li><code>&lt;deployment-or-rollout-name&gt;</code>: Replace with name of the rollout or the deployment for the service. This will be scaled up to min-target-replicas when first request comes</li> <li><code>cooldownPeriod</code>: Minimum time (in seconds) to wait after scaling up before considering scale down. <ul> <li>Default: 900 seconds (15 minutes)</li> <li>Maximum: 604800 seconds (7 days)</li> <li>Minimum: 1 seconds (1 second)</li> </ul> </li> <li><code>triggers</code>: List of conditions that determine when to scale down (currently supports only Prometheus metrics)</li> <li><code>autoscaler</code>: Optional integration with an external autoscaler (HPA/KEDA) if needed<ul> <li><code>&lt;autoscaler-type&gt;</code>: keda</li> <li><code>&lt;autoscaler-object-name&gt;</code>: Name of the KEDA ScaledObject</li> </ul> </li> </ul>"},{"location":"src/gs-configure-elastiservice/#configuration-explanation","title":"Configuration Explanation","text":"<p>The section below explains how the different configuration options are used in KubeElasti.</p>"},{"location":"src/gs-configure-elastiservice/#1-scaletargetref-which-service-kubeelasti-should-manage","title":"1. scaleTargetRef: Which service KubeElasti should manage","text":"<p>This is defined using the <code>scaleTargetRef</code> field in the spec. </p> <ul> <li><code>scaleTargetRef.kind</code>: should be either be  <code>deployments</code> or <code>rollouts</code> (in case you are using Argo Rollouts). </li> <li><code>scaleTargetRef.apiVersion</code> will be <code>apps/v1</code> if you are using deployments or <code>argoproj.io/v1alpha1</code> in case you are using argo-rollouts. </li> <li><code>scaleTargetRef.name</code> should exactly match the name of the deployment or rollout. </li> </ul> <p></p>"},{"location":"src/gs-configure-elastiservice/#2-triggers-when-to-scale-down-the-service-to-0","title":"2. Triggers: When to scale down the service to 0","text":"<p>This is defined using the <code>triggers</code> field in the spec. Currently, KubeElasti supports only one trigger type - <code>prometheus</code>.  The <code>metadata</code> section holds trigger-specific data:  </p> <ul> <li>query - the Prometheus query to evaluate  </li> <li>serverAddress - address of the Prometheus server  </li> <li>threshold - numeric threshold that triggers scale-down  </li> </ul> <p>For example, you can query the number of requests per second and set the threshold to <code>0</code>. KubeElasti polls this metric every 30 seconds, and if the value is below the threshold it scales the service to 0.</p> <p>An example trigger is as follows:</p> <pre><code>triggers:\n- type: prometheus\n    metadata:\n    query: sum(rate(nginx_ingress_controller_nginx_process_requests_total[1m])) or vector(0)\n    serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090\n    threshold: 0.5\n</code></pre> <p></p>"},{"location":"src/gs-configure-elastiservice/#3-scalers-how-to-scale-up-the-service-to-1","title":"3. Scalers: How to scale up the service to 1","text":"<p>Once the service is scaled down to 0, we also need to pause the current autoscaler to make sure it doesn't scale up the service again. While this is not a problem with HPA, Keda will scale up the service again since the min replicas is 1. Hence, KubeElasti needs to know about the KEDA ScaledObject so that it can pause it. This information is provided in the <code>autoscaler</code> field of the ElastiService. Currently, the only supported autoscaler type is keda.</p> <pre><code>autoscaler:\n  name: &lt;autoscaler-object-name&gt;\n  type: keda\n</code></pre> <p></p>"},{"location":"src/gs-configure-elastiservice/#4-cooldownperiod-minimum-time-in-seconds-to-wait-after-scaling-up-before-considering-scale-down","title":"4. CooldownPeriod: Minimum time (in seconds) to wait after scaling up before considering scale down","text":"<p>As soon as the service is scaled down to 0, KubeElasti resolver will start accepting requests for that service. On receiving the first request, it will scale up the service to <code>minTargetReplicas</code>. Once the pod is up, the new requests are handled by the service pods and do not pass through the elasti-resolver. The requests that came before the pod scaled up are held in memory of the elasti-resolver and are processed once the pod is up.</p> <p>We can configure the <code>cooldownPeriod</code> to specify the minimum time (in seconds) to wait after scaling up before considering scale down.</p>"},{"location":"src/gs-introduction/","title":"Introduction","text":"<p>KubeElasti (Sometimes referred to as just \"Elasti\") is a Kubernetes-native solution that offers scale-to-zero functionality when there is no traffic and automatically scales up from 0 when traffic arrives. Most Kubernetes autoscaling solutions like HPA or Keda can scale from 1 to n replicas based on cpu utilization or memory usage. However, these solutions do not offer a way to scale to 0 when there is no traffic. KubeElasti solves this problem by dynamically managing service replicas based on real-time traffic conditions. It only handles scaling the application down to 0 replicas and scaling it back up to 1 replica when traffic is detected again. The scaling after 1 replica is handled by the autoscaler like HPA or Keda.</p> <p>Info</p> <p>The name Elasti comes from a superhero \"Elasti-Girl\" from DC Comics. Her superpower is to expand or shrink her body at will\u2014from hundreds of feet tall to mere inches in height. \"Kube\" refers to Kubernetes, while \"Elasti\" highlights elastic scaling super-powers.</p> <p>KubeElasti uses a proxy mechanism that queues and holds requests for scaled-down services, bringing them up only when needed. The proxy is used only when the service is scaled down to 0. When the service is scaled up to 1, the proxy is disabled and the requests are processed directly by the pods of the service.</p>"},{"location":"src/gs-introduction/#how-it-works","title":"How It Works","text":"<p>KubeElasti continuously monitors an ElastiService by evaluating a set of custom triggers defined in its configuration. These triggers represent various conditions\u2014such as traffic metrics or other custom signals\u2014that determine whether a service should be active or scaled down.</p> <ul> <li> <p>Scaling Down:   When all triggers indicate inactivity or low demand, KubeElasti scales the target service down to 0 replicas. During this period, KubeElasti switches into proxy mode and queues incoming requests instead of dropping them.</p> </li> <li> <p>Traffic Queueing in Proxy Mode:   In Proxy Mode, KubeElasti intercepts and queues incoming requests directed at the scaled-down service. This ensures that no request is lost, even when the service is scaled down to 0.</p> </li> <li> <p>Scaling Up:   If any trigger signals a need for activity, KubeElasti immediately scales the service back up to its minimum replicas. As the service comes online, KubeElasti switches to Serve Mode.</p> </li> <li> <p>Serve Mode:   In Serve Mode, the active service handles all incoming traffic directly. Meanwhile, any queued requests accumulated during Proxy Mode are processed, ensuring a seamless return to full operational capacity.</p> </li> </ul> <p>This allows KubeElasti to optimize resource consumption by scaling services down when unneeded, while its request queueing mechanism preserves user interactions and guarantees prompt service availability when conditions change.</p> <pre><code>---\ntitle: Lifecycle modes of KubeElasti (Proxy, Serve)\ndisplayMode: compact\nconfig:\n  layout: elk\n  look: classic\n  theme: dark\n---\n\nflowchart TB\n    %% Proxy Mode\n    subgraph Proxy_Mode[\"Proxy Mode | Pods = 0\"]\n        direction TB\n        Service1[Service]\n        Proxy[Elasti Proxy]\n        Pod1[Pod]\n\n        Service1 --&gt;|Traffic| Proxy\n        Proxy --&gt;|1: Queue Req if Pod = 0| Proxy\n        Proxy --&gt;|2: Scale| Pod1\n        Proxy --&gt;|3: Send Req| Pod1\n    end\n\n    %% Serve Mode\n    subgraph Serve_Mode[\"Serve Mode | Pods \u2265 1\"]\n        direction TB\n        Service2[Service]\n        Pod2[Pod]\n\n        Service2 --&gt;|1: Req Sent| Pod2\n    end\n\n\n\n</code></pre>"},{"location":"src/gs-introduction/#key-features","title":"Key Features","text":"<ul> <li> <p>Seamless Integration: KubeElasti integrates effortlessly with your existing Kubernetes setup-whether you are using HPA or Keda. It takes just a few steps to enable scale to zero for any service.</p> </li> <li> <p>Deployment and Argo Rollouts Support: KubeElasti supports two scale target references: Deployment and Argo Rollouts, making it versatile for various deployment scenarios.</p> </li> <li> <p>Prometheus Metrics Export: KubeElasti exports Prometheus metrics for easy out-of-the-box monitoring. You can also import a pre-built dashboard into Grafana for comprehensive visualization.</p> </li> <li> <p>Generic Service Support: KubeElasti works at the kubernetes service level. It also supports East-West traffic using cluster-local service DNS, ensuring robust and flexible traffic management across your services. So any ingress or service mesh solution can be used with KubeElasti.</p> </li> </ul>"},{"location":"src/gs-introduction/#limitations","title":"Limitations","text":"<ul> <li>Only HTTP is supported: KubeElasti currently supports requests that are routed to the service via HTTP. In the future we will support more protocols like TCP, UDP etc.</li> <li>Only Deployment and Argo Rollouts are supported: KubeElasti supports two scale target references: Deployment and Argo Rollouts. In the future this will be made generic to support all target references that support the <code>/scale</code> subresource.</li> <li>Prometheus Trigger: The only trigger currently supported is Prometheus</li> </ul> <p>Please checkout the comparison here to see how KubeElasti compares to other Kubernetes autoscaling solutions.</p>"},{"location":"src/gs-scalers/","title":"Scalers","text":""},{"location":"src/gs-scalers/#scaling-with-hpa","title":"Scaling with HPA","text":"<p>KubeElasti works seamlessly with the Horizontal Pod Autoscaler (HPA) and handles scaling to zero on its own. Since KubeElasti manages the scale-to-zero functionality, you can configure HPA to handle scaling based on metrics for any number of replicas greater than zero, while KubeElasti takes care of scaling to/from zero.</p> <p>A setup is explained in the getting started guide.</p>"},{"location":"src/gs-scalers/#scaling-with-keda","title":"Scaling with KEDA","text":"<p>KubeElasti takes care of scaling up and down a service when there is some traffic. KEDA is a good candidate for performing the scaling logic for the service from minReplicas to maxReplicas based on its triggers.</p> <p>Here we will see how to integrate KubeElasti with KEDA to build a complete scaling solution.</p>"},{"location":"src/gs-scalers/#prerequisites","title":"Prerequisites","text":"<ul> <li>Make sure you have gone through the getting started guide. We will extend the same setup for this integration.</li> <li>KEDA installed in the cluster - KEDA Installation</li> </ul>"},{"location":"src/gs-scalers/#steps","title":"Steps","text":""},{"location":"src/gs-scalers/#1-create-a-keda-scaler-for-the-service","title":"1. Create a keda scaler for the service","text":"<p>Let's create a keda scaler for the httpbin service.</p> <p><pre><code>kubectl apply -f ./playground/config/demo-application-keda.yaml\n</code></pre> Note that the same prometheus query is used as in the getting started guide for ElastiService and the namespace is the same as the namespace that the ElastiService is created in.</p> <p>Refer to the keda documentation for more details on configuring the ScaledObject.</p>"},{"location":"src/gs-scalers/#2-update-elastiservice-to-work-with-the-keda-scaler","title":"2. Update ElastiService to work with the keda scaler","text":"<p>We will update the ElastiService to specify the keda scaler to work with. We will add the following fields to the ElastiService object: <pre><code>spec:\n  autoscaler:\n    name: httpbin-scaled-object\n    type: keda\n</code></pre></p> <p>Patch the ElastiService object with the above changes.</p> <pre><code>kubectl patch elastiservice httpbin-elasti -n elasti-demo -p '{\"spec\":{\"autoscaler\":{\"name\": \"httpbin-scaled-object\", \"type\": \"keda\"}}}' --type=merge\n</code></pre> <p>Now when KubeElasti scales down the service, it will pause the keda ScaledObject to prevent it from scaling up the service again, and when KubeElasti scales up the service, it will resume the ScaledObject.</p> <p>With these changes, KubeElasti can reliably scale up the service when there is traffic and scale down the service to zero when there is no traffic while keda can handle the scaling logic for the service from minReplicas to maxReplicas based on its triggers.</p>"},{"location":"src/gs-setup/","title":"Setup","text":"<p>Get started by following below steps:</p>"},{"location":"src/gs-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes Cluster: You should have a running Kubernetes cluster. You can use any cloud-based or on-premises Kubernetes distribution.</li> <li>kubectl: Installed and configured to interact with your Kubernetes cluster.</li> <li>Helm: Installed for managing Kubernetes applications.</li> <li>Prometheus: You should have a prometheus installed in your cluster.</li> </ul> Installing Prometheus <p>We will setup a sample prometheus to read metrics from the ingress controller.</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --set alertmanager.enabled=false \\\n  --set grafana.enabled=false \\\n  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false\n</code></pre> <ul> <li>Ingress Controller: You should have an ingress controller installed in your cluster.</li> </ul> Installing Ingress Controller NGINXIstio <pre><code>  helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n  helm repo update\n  helm upgrade --install nginx-ingress ingress-nginx/ingress-nginx \\\n    --namespace nginx \\\n    --set controller.metrics.enabled=true \\\n    --set controller.metrics.serviceMonitor.enabled=true \\\n    --create-namespace\n</code></pre> <pre><code># Download the latest Istio release from the official Istio website.\ncurl -L https://istio.io/downloadIstio | sh -\n# Move it to home directory\nmv istio-x.xx.x ~/.istioctl\nexport PATH=$HOME/.istioctl/bin:$PATH\n\nistioctl install --set profile=default -y\n\n# Label the namespace where you want to deploy your application to enable Istio sidecar Injection\nkubectl create namespace &lt;NAMESPACE&gt;\nkubectl label namespace &lt;NAMESPACE&gt; istio-injection=enabled\n\n# Create a gateway\nkubectl apply -f ./playground/config/gateway.yaml -n &lt;NAMESPACE&gt;\n</code></pre> <ul> <li>KEDA*: [Optional] You can have a KEDA installed in your cluster, else HPA can be used.</li> </ul> Installing KEDA <p>We will setup a sample KEDA to scale the target deployment.</p> <pre><code>helm repo add kedacore https://kedacore.github.io/charts\nhelm repo update\nhelm upgrade --install keda kedacore/keda --namespace keda --create-namespace --wait --timeout 180s\n</code></pre>"},{"location":"src/gs-setup/#install","title":"Install","text":""},{"location":"src/gs-setup/#1-install-kubeelasti-using-helm","title":"1. Install KubeElasti using helm","text":"<p>Use Helm to install KubeElasti into your Kubernetes cluster. </p> <pre><code>helm install elasti oci://tfy.jfrog.io/tfy-helm/elasti --namespace elasti --create-namespace\n</code></pre> <p>Check out values.yaml to see configuration options in the helm value file.</p> <p></p>"},{"location":"src/gs-setup/#2-verify-the-installation","title":"2. Verify the Installation","text":"<p>Check the status of your Helm release and ensure that the KubeElasti components are running:</p> <pre><code>helm status elasti --namespace elasti\nkubectl get pods -n elasti\n</code></pre> <p>You will see 2 components running.</p> <ol> <li>Controller/Operator: <code>elasti-operator-controller-manager-...</code> is to switch the traffic, watch resources, scale etc.</li> <li>Resolver: <code>elasti-resolver-...</code> is to proxy the requests.</li> </ol> <p></p>"},{"location":"src/gs-setup/#3-define-an-elastiservice","title":"3. Define an ElastiService","text":"<p>To configure a service to handle its traffic via elasti, you'll need to create and apply a <code>ElastiService</code> custom resource.</p> <p>Here we are creating it for httpbin service.   </p> <p>Create a file named <code>elasti-service.yaml</code> and apply the configuration.</p> elasti-service.yaml<pre><code>apiVersion: elasti.truefoundry.com/v1alpha1\nkind: ElastiService\nmetadata:\n  name: &lt;service-name&gt; # (1)\n  namespace: &lt;service-namespace&gt; # (2)\nspec:\n  minTargetReplicas: &lt;min-target-replicas&gt; # (3)\n  service: &lt;service-name&gt;\n  cooldownPeriod: &lt;cooldown-period&gt; # (4)\n  scaleTargetRef:\n    apiVersion: &lt;apiVersion&gt; # (5)\n    kind: &lt;kind&gt; # (6)\n    name: &lt;deployment-or-rollout-name&gt; # (7)\n  triggers:\n  - type: &lt;trigger-type&gt; # (8)\n    metadata:\n      query: &lt;query&gt; # (9)\n      serverAddress: &lt;server-address&gt; # (10)\n      threshold: &lt;threshold&gt; # (11)\n      uptimeFilter: &lt;uptime-filter&gt; # (12)\n  autoscaler:\n    name: &lt;autoscaler-object-name&gt; # (13)\n    type: &lt;autoscaler-type&gt; # (14)\n</code></pre> <ol> <li>Replace it with the service you want managed by elasti.</li> <li>Replace it with the namespace of the service.</li> <li>Replace it with the min replicas to bring up when first request arrives. Minimum: 1</li> <li>Replace it with the cooldown period to wait after scaling up before considering scale down. Default: 900 seconds (15 minutes) | Maximum: 604800 seconds (7 days) | Minimum: 1 second (1 second)</li> <li>ApiVersion should be <code>apps/v1</code> if you are using deployments or <code>argoproj.io/v1alpha1</code> in case you are using argo-rollouts. </li> <li>Kind should be either <code>Deployment</code> or <code>Rollout</code> (in case you are using Argo Rollouts).</li> <li>Name should exactly match the name of the deployment or rollout.</li> <li>Replace it with the trigger type. Currently, KubeElasti supports only one trigger type - <code>prometheus</code>. </li> <li>Replace it with the trigger query. In this case, it is the number of requests per second.</li> <li>Replace it with the trigger server address. In this case, it is the address of the prometheus server.</li> <li>Replace it with the trigger threshold. In this case, it is the number of requests per second.</li> <li>Replace it with the uptime filter of your TSDB instance. Default: <code>container=\"prometheus\"</code>.</li> <li>Replace it with the autoscaler name. In this case, it is the name of the KEDA ScaledObject.</li> <li>Replace it with the autoscaler type. In this case, it is <code>keda</code>.</li> </ol> Demo ElastiService elasti-service.yaml<pre><code>apiVersion: elasti.truefoundry.com/v1alpha1\nkind: ElastiService\nmetadata:\n  name: target-elastiservice\n  namespace: target\nspec:\n  cooldownPeriod: 5\n  minTargetReplicas: 1\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: deployments\n    name: target-deployment\n  service: target-deployment\n  triggers:\n    - metadata:\n        query: round(sum(rate(envoy_http_downstream_rq_total{container=\"istio-proxy\"}[1m])),0.001) or vector(0)\n        serverAddress: http://prometheus-operated.monitoring.svc.cluster.local:9090\n        threshold: \"0.01\"\n      type: prometheus\n  autoscaler:\n    name: target-scaled-object\n    type: keda\n</code></pre> <p></p>"},{"location":"src/gs-setup/#4-apply-the-kubeelasti-service-configuration","title":"4. Apply the KubeElasti service configuration","text":"<p>Apply the configuration to your Kubernetes cluster:</p> <pre><code>kubectl apply -f elasti-service.yaml -n &lt;service-namespace&gt;\n</code></pre> <p>The pod will be scaled down to 0 replicas if there is no traffic.</p> <p></p>"},{"location":"src/gs-setup/#5-test-the-setup","title":"5. Test the setup","text":"<p>You can test the setup by sending requests to the nginx load balancer service.</p> <pre><code># For NGINX\nkubectl port-forward svc/nginx-ingress-ingress-nginx-controller -n nginx 8080:80\n\n# For Istio\nkubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\n</code></pre> <p>Start a watch on the target deployment.</p> <pre><code>kubectl get pods -n &lt;NAMESPACE&gt; -w\n</code></pre> <p>Send a request to the service.</p> <pre><code>curl -v http://localhost:8080/httpbin\n</code></pre> <p>You should see the pods being created and scaled up to 1 replica. A response from the   target service should be visible for the curl command. The target service should be scaled down to 0 replicas if there is no traffic for <code>cooldownPeriod</code> seconds.</p> <p></p>"},{"location":"src/gs-setup/#uninstall","title":"Uninstall","text":"<p>To uninstall Elasti, you will need to remove all the installed ElastiServices first. Then, simply delete the installation file.</p> <pre><code>kubectl delete elastiservices --all\nhelm uninstall elasti -n elasti\nkubectl delete namespace elasti\n</code></pre>"},{"location":"src/gs-triggers/","title":"Triggers","text":""},{"location":"src/gs-triggers/#trigger-with-prometheus","title":"Trigger with Prometheus","text":"<p>ElastiService uses Prometheus queries to determine when to scale down services to zero. This section provides guidance on how to create effective queries for your ElastiService configuration.</p>"},{"location":"src/gs-triggers/#finding-the-right-prometheus-query","title":"Finding the Right Prometheus Query","text":"<ol> <li> <p>Identify the Metric: First, determine which metric best represents your service activity. Common metrics include:</p> <ul> <li>HTTP request rates (<code>requests_total</code>, <code>request_count</code>)</li> <li>Connection counts</li> <li>Processing rates</li> <li>Custom application metrics</li> </ul> </li> <li> <p>Access Prometheus UI: </p> <ul> <li>Connect to your Prometheus instance (typically available at <code>http://&lt;prometheus-service&gt;.&lt;namespace&gt;.svc.cluster.local:9090</code>)</li> <li>For local development, you may need to port-forward: <code>kubectl port-forward svc/prometheus-server -n monitoring 9090:9090</code></li> </ul> </li> <li> <p>Explore Available Metrics:</p> <ul> <li>In the Prometheus UI, go to the \"Graph\" tab</li> <li>Click on the \"Insert metric at cursor\" dropdown to see available metrics</li> <li>Filter by typing part of your service name</li> </ul> </li> <li> <p>Test Your Query:</p> <ul> <li>For HTTP request rates, a common pattern is: <code>sum(rate(http_requests_total{service=\"your-service\"}[1m]))</code></li> <li>For Nginx Ingress: <code>sum(rate(nginx_ingress_controller_requests_total{service=\"your-service\"}[1m]))</code></li> <li>Always add a fallback: <code>or vector(0)</code> to handle cases with no data</li> </ul> </li> <li> <p>Visualize and Refine:</p> <ul> <li>Execute the query in the Prometheus UI</li> <li>Adjust the time range to see historical patterns</li> <li>Refine labels and functions until you get the expected results</li> </ul> </li> </ol>"},{"location":"src/gs-triggers/#example-queries","title":"Example Queries","text":"NGINX Ingress Controller RequestsIstio Request CountCustom Application Metric <pre><code>sum(rate(nginx_ingress_controller_requests_total{namespace=\"your-namespace\", ingress=\"your-ingress\"}[1m])) or vector(0)\n</code></pre> <pre><code>sum(rate(istio_requests_total{destination_service_name=\"your-service\"}[1m])) or vector(0)\n</code></pre> <pre><code>sum(rate(app_metric_name{service=\"your-service\"}[1m])) or vector(0)\n</code></pre>"},{"location":"src/gs-triggers/#setting-the-threshold","title":"Setting the Threshold","text":"<p>The <code>threshold</code> value in your ElastiService configuration determines when scaling to zero occurs:</p> <ul> <li>If your query returns the request rate per second, a threshold of <code>0.5</code> means scale to zero when fewer than 0.5 requests per second occur</li> <li>For absolute counts, set the threshold accordingly (e.g., <code>5</code> for 5 total connections)</li> <li>Consider setting a non-zero threshold (like <code>0.1</code>) to provide a buffer before scaling down</li> </ul>"},{"location":"src/gs-triggers/#complete-trigger-configuration-example","title":"Complete Trigger Configuration Example","text":"<pre><code>triggers:\n- type: prometheus\n  metadata:\n    query: sum(rate(nginx_ingress_controller_requests_total{namespace=\"your-namespace\", service=\"your-service\"}[1m])) or vector(0)\n    serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090\n    threshold: 0.5\n</code></pre>"},{"location":"src/gs-triggers/#verifying-your-query","title":"Verifying Your Query","text":"<p>Before applying your ElastiService configuration:</p> <ol> <li>Test the query in Prometheus UI during both active and inactive periods</li> <li>Ensure it returns numeric values (not errors or empty results)</li> <li>Verify the query captures all relevant traffic to your service</li> <li>Check that the <code>or vector(0)</code> fallback works when there's no data</li> </ol>"},{"location":"src/gs-triggers/#common-query-patterns","title":"Common Query Patterns","text":"Metric Source Query Pattern Nginx Ingress <code>sum(rate(nginx_ingress_controller_requests_total{namespace=\"ns\", ingress=\"name\"}[1m])) or vector(0)</code> Istio <code>sum(rate(istio_requests_total{destination_service_name=\"name\"}[1m])) or vector(0)</code> Kubernetes API Server <code>sum(apiserver_request_total{resource=\"your-resource\"}) or vector(0)</code> Custom App Metric <code>sum(rate(app_metric_name{service=\"name\"}[1m])) or vector(0)</code>"},{"location":"src/message/","title":"Message","text":"<pre><code>graph TB\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  %%  SUBGRAPHS (logical zones)\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  subgraph INGRESS [\" \"]\n    Gateway[Gateway]\n  end\n\n\n\n  subgraph CONTROL_PLANE [\"KubeElasti\"]\n    Operator[Operator]\n    Resolver[Resolver]\n  end\n\n  LoadGen[Internal-load-generator-pod]\n\n  subgraph ELASTI_CRD [\"CRDs\"]\n    ESCRD((ElastiService&lt;br&gt;CRD))\n  end\n\n\n\n    TargetSVC{Target-SVC}\n    TargetSVC_PVT{Target\u2011SVC&lt;br&gt;Private}\n\n\n  subgraph ENDPOINT [\"Endpoints\"]\n    SVC_EP([target-SVC&lt;br&gt;endpoints])\n    SVC_EPS([target-SVC-y2b93&lt;br&gt;endpointSlice])\n    ResEPS([target-SVC-to-resolver&lt;br&gt;endpointSlice])\n  end\n\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  %%  TRAFFIC FLOWS (solid)\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  Gateway --&gt;|1: traffic| TargetSVC\n  LoadGen --&gt;|1: traffic| TargetSVC\n\n  %% Serve\u2011mode path\n  TargetSVC --&gt;|2: Serve Mode| SVC_EP\n  SVC_EP --&gt; SVC_EPS\n  SVC_EPS --&gt; Pod\n\n  %% Proxy\u2011mode path\n  TargetSVC --&gt;|7: Proxy Mode| ResEPS\n  ResEPS --&gt;|9: Req| Resolver\n  Resolver --&gt;|11: Send Proxy Request&lt;br&gt;once pod ready| TargetSVC_PVT\n  TargetSVC_PVT --&gt;|12: Send and Receive Req| Pod\n\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  %%  OPERATOR / WATCH (dashed)\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  ESCRD -. \"0: Watch CRD\" .-&gt; Operator\n  Operator -. \"0: Watch ScaleTargetRef\" .-&gt; Operator\n  Operator -. \"4: When target scaled to 0\" .-&gt; Operator\n  Operator -. \"4: Create + add resolver POD IPs\" .-&gt; ResEPS\n  Operator -. \"5: Watch Resolver \u2192 endpointslice\" .-&gt; Resolver\n  Operator -. \"6: Create PVT SVC\" .-&gt; TargetSVC_PVT\n  Operator -. \"7: Watch public SVC \u2192 private SVC\" .-&gt; TargetSVC\n  Operator -. \"12: Send Traffic Info\" .-&gt; Resolver\n  Resolver -. \"13: Scale ScaleTargetRef&lt;br&gt; and reverse 4,5\" .-&gt; Operator\n\n  Pod -. \"3: Pod scaled to 0 via HPA/KEDA\" .-&gt; Operator\n</code></pre> <pre><code>---\ntitle: KubeElasti Architecture\ndisplayMode: compact\nconfig:\n  layout: elk\n  look: classic\n  theme: default\n  securityLevel: loose\n  flowchart:\n    nodeSpacing: 30\n    rankSpacing: 40\n  fontFamily: \"Inter, sans-serif\"\n  themeVariables:\n    fontSize: \"14px\"\n---\n\ngraph LR\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  %%  SUBGRAPHS (logical zones)\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  subgraph INGRESS [\" \"]\n    Gateway[Gateway]\n  end\n\n\n\n  subgraph CONTROL_PLANE [\"KubeElasti\"]\n    Operator[Operator]\n    Resolver[Resolver]\n  end\n\n  LoadGen[Internal-load-generator-pod]\n\n  subgraph ELASTI_CRD [\"CRDs\"]\n    ESCRD((ElastiService&lt;br&gt;CRD))\n  end\n\n\n\n    TargetSVC{Target-SVC}\n    TargetSVC_PVT{Target\u2011SVC&lt;br&gt;Private}\n\n\n  subgraph ENDPOINT [\"Endpoints\"]\n    SVC_EP([target-SVC&lt;br&gt;endpoints])\n    SVC_EPS([target-SVC-y2b93&lt;br&gt;endpointSlice])\n    ResEPS([target-SVC-to-resolver&lt;br&gt;endpointSlice])\n  end\n\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  %%  TRAFFIC FLOWS (solid)\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  Gateway --&gt;|1: traffic| TargetSVC\n  LoadGen --&gt;|1: traffic| TargetSVC\n\n  %% Serve\u2011mode path\n  TargetSVC --&gt; SVC_EP\n  SVC_EP --&gt;|2: Serve Mode| SVC_EPS\n  SVC_EPS --&gt; Pod\n\n  %% Proxy\u2011mode path\n  SVC_EP --&gt;|7: Proxy Mode| ResEPS\n  ResEPS --&gt;|9: Req| Resolver\n  Resolver --&gt;|11: Send Proxy Request&lt;br&gt;once pod ready| TargetSVC_PVT\n  TargetSVC_PVT --&gt;|12: Send and Receive Req| Pod\n\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  %%  OPERATOR / WATCH (dashed)\n  %% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  ESCRD -. \"0: Watch CRD\" .-&gt; Operator\n  Operator -. \"0: Watch ScaleTargetRef\" .-&gt; Operator\n  Operator -. \"4: When target scaled to 0\" .-&gt; Operator\n  Operator -. \"4: Create + add resolver POD IPs\" .-&gt; ResEPS\n  Operator -. \"5: Watch Resolver \u2192 endpointslice\" .-&gt; Resolver\n  Operator -. \"6: Create PVT SVC\" .-&gt; TargetSVC_PVT\n  Operator -. \"7: Watch public SVC \u2192 private SVC\" .-&gt; TargetSVC\n  Operator -. \"12: Send Traffic Info\" .-&gt; Resolver\n  Resolver -. \"13: Scale ScaleTargetRef&lt;br&gt; and reverse 4,5\" .-&gt; Operator\n\n  Pod -. \"3: Pod scaled to 0 via HPA/KEDA\" .-&gt; Operator\n</code></pre>"},{"location":"blog/archive/2025/","title":"2025","text":""}]}